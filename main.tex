\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{listings}
\lstset{basicstyle=\ttfamily\small,breaklines=true}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Automated Documentation Generation and Multi-Stage Evaluation Framework for LLM Comprehension of Emerging Programming Languages}

\author{\IEEEauthorblockN{Shengran Jin\IEEEauthorrefmark{1}\IEEEauthorrefmark{2} and
Ibrahim Musaddequr Rahman\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}University of Michigan}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Email: [Email Placeholder]}
\thanks{This work was conducted at the University of Michigan.}
}

\maketitle

\begin{abstract}
Large Language Models cannot generate accurate code for languages absent from their training data. We present an automated documentation generation pipeline and a two-stage evaluation framework for creating and validating LLM-optimized documentation at scale. The generation pipeline systematically processes large documentation sets through topic extraction, hierarchical merging, and compression, producing token-efficient references in 30-45 minutes. The evaluation framework validates generated code through pattern matching and compiler-based syntax checking. Applied to the Jac programming language, our unified documentation (2,612 tokens) achieved 68-73\% accuracy across six frontier models. We demonstrate that compiler-based validation catches errors that pattern matching alone cannot detect, and that documentation size exhibits a nonlinear relationship with comprehension quality. We provide open-source tooling for reproducible evaluation and iterative refinement of documentation for emerging languages.
\end{abstract}

\section{Introduction}

LLM-powered coding assistants struggle with languages released after their training cutoff, posing a barrier to emerging language ecosystems. While manual documentation generation can produce token-efficient references, it does not scale to large documentation sets and lacks systematic evaluation methods.

This paper presents two primary contributions: (1) an automated multi-stage pipeline for documentation generation that processes hundreds of source files into token-efficient references, and (2) a two-stage evaluation framework that validates code correctness through pattern matching and compiler-based syntax checking. We demonstrate that compiler validation catches errors that pattern matching alone cannot detect, and that documentation size exhibits a nonlinear relationship with comprehension quality. The framework's utility is demonstrated through its application to the Jac programming language, achieving 68-73\% accuracy across six frontier LLMs.

\section{Problem Definition}

Given a programming language $L$ for which LLMs have limited pre-training knowledge and documentation $D$ describing $L$, we seek to quantify the LLM's ability to generate correct code in $L$ after reading $D$, identify weaknesses in $D$ to guide improvement, and compare documentation variants to optimize the accuracy/token trade-off.

We define the evaluation objective as a scoring function $S(D, M, T)$ where $D$ is documentation, $M$ is an LLM, and $T$ is a test suite:

\begin{equation}
    S(D, M, T) = \sum_{t \in T} \text{score}(M(D, t), t)
\end{equation}

where $M(D, t)$ is the code generated by model $M$ given documentation $D$ and test prompt $t$.

\section{Evaluation Framework}

\subsection{Test Suite Design}

We developed a comprehensive evaluation suite consisting of 120 tests designed to measure LLM comprehension of the Jac language. The suite covers distinct categories across 10 difficulty levels, with tests weighted by difficulty (Level 1 = 5 points, Level 10 = 20 points, max 3900 points total). Tests span basic syntax to production-grade scenarios involving graph operations, walkers, and AI integration.

\subsection{Two-Stage Validation Pipeline}

Each generated code response undergoes two validation stages to ensure correctness:

\textbf{Stage 1: Pattern Matching} ($\sim$10ms per test). Each test defines required and forbidden element sets. Required elements specify syntax patterns that must appear (e.g., \texttt{walker} keyword and \texttt{visit} syntax for walker tests). Forbidden elements identify patterns indicating structural misunderstanding (e.g., writing a function when a walker was requested). This stage provides rapid structural validation before expensive compilation.

\textbf{Stage 2: Syntax Validation} (200-500ms per test). Generated code is written to a temporary file and validated using the Jac compiler (\texttt{jac check}). Compiler errors result in test failure. This stage handles LLM output variations (markdown code blocks, explanatory text) by extracting and validating only the code portion. It catches syntactic errors that pattern matching cannot detect: mismatched brackets, incorrect indentation, and malformed expressions.

The evaluation framework includes infrastructure for functional testing (\texttt{jac test} with test harnesses), but the current 120-test suite focuses on syntax validation to ensure generated code compiles correctly. Future work includes expanding the test suite to include functional validation for semantic correctness.

\subsection{Scoring Methodology}

Tests receive:
\begin{itemize}
    \item Full points: Passed both pattern matching and syntax validation
    \item Partial credit: Passed pattern matching with partial required element coverage
    \item Zero: Failed pattern matching or syntax validation
\end{itemize}

Penalties are applied proportionally for missing required elements (up to 100\% of points), forbidden elements (up to 30\% of points), and syntax errors (15\% penalty). Scores are aggregated by category and difficulty level to identify documentation weaknesses.

\section{Documentation Generation Pipeline}

Manual documentation generation does not scale to large documentation sets spanning hundreds of files. We developed a five-stage automated pipeline that systematically processes source documentation into token-efficient references.

\subsection{Pipeline Architecture}

\textbf{Stage 1: Topic Extraction}. The system defines 40+ topics covering language features (syntax, types, functions, objects, nodes, edges, walkers, AI integration, standard library). Instead of making one LLM call per topic per source file ($O(n \times m)$ calls for $n$ files and $m$ topics), the extractor sends each file once and extracts all matching topics simultaneously using structured output markers. This single-pass extraction reduces API costs by 10-30x. Topic-specific content is accumulated into separate files using thread-safe file operations.

\textbf{Stage 2: Topic Merging}. Scattered snippets for each topic are synthesized into cohesive single-topic guides. Large topics ($>$20KB) undergo recursive chunking: the system splits by section headers, merges chunks in parallel, then recursively merges results until reaching a manageable size. Stagnation detection prevents infinite loops when compression fails. Protection rules in prompts preserve critical APIs and syntax variants.

\textbf{Stage 3: Hierarchical Merging}. The system combines $\sim$40 disjoint topic files into a unified document through multi-pass merging with a configurable merge ratio (e.g., 2:1). This produces a hierarchical structure: 40 files $\rightarrow$ 20 files $\rightarrow$ 10 files $\rightarrow$ ... $\rightarrow$ 1 unified document. Multi-pass processing prevents overwhelming the LLM with too many topics simultaneously while maintaining cross-topic coherence.

\textbf{Stage 4: Final Compression}. Two-phase compression applies LLM-based formatting (removing explanatory fluff while preserving syntax) followed by regex minification (stripping unnecessary whitespace). Headers, code blocks, and lists remain intact for readability.

\textbf{Stage 5: Release}. The final artifact is versioned for evaluation. The complete pipeline executes in 30-45 minutes using Gemini 2.5 Flash, producing deterministic output: identical source and configuration yield identical documentation.

\subsection{Benchmark-Driven Refinement}

The evaluation framework enables iterative improvement:

\begin{enumerate}
    \item Execute benchmark suite on current documentation
    \item Identify lowest-scoring categories from aggregated results
    \item Generate targeted documentation for weak areas using the pipeline
    \item Integrate patches and re-run evaluation to validate improvement
\end{enumerate}

\subsection{Documentation Versions}

The pipeline produced three documentation versions:

\begin{table}[htbp]
\caption{Documentation Versions}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Version} & \textbf{Size} & \textbf{Tokens} \\
\midrule
Mini v3 & 4.2 KB & 1,073 \\
Core v3 & 33 KB & 8,340 \\
Unified v4.9 & 11 KB & 2,612 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

The unified v4.9 documentation represents a middle ground between aggressive compression (mini) and comprehensive explanation (core), produced through the complete five-stage pipeline.

\section{Experimental Results}

\subsection{Evaluation Protocol}

All evaluations use stateless queries via OpenRouter, a unified API gateway that routes requests to various LLM providers. Each test is submitted independently without conversational context, ensuring reproducible and isolated measurements. Tests are executed in configurable batch sizes, with results aggregated across multiple runs to account for variance.

\subsection{Unified Documentation Performance}

The v4.9 unified documentation (2,612 tokens) was evaluated across six frontier models using the 120-test suite (max 3900 points):

\begin{table}[htbp]
\caption{Cross-Model Performance (v4.9 Unified Documentation)}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Score} & \textbf{Accuracy} \\
\midrule
Claude Sonnet 4.5 & 2843 & 72.9\% \\
Claude Opus 4.5 & 2823 & 72.4\% \\
Gemini 2.5 Flash & 2647 & 67.9\% \\
GPT-5 Mini & 2646 & 67.8\% \\
GPT-5.1 & 2625 & 67.3\% \\
Gemini 3 Pro & 2620 & 67.2\% \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Claude models demonstrate superior performance (72-73\%), leading other architectures by 5+ percentage points. Notably, performance within model families (e.g., GPT-5 Mini vs GPT-5.1, Gemini Flash vs Gemini Pro) shows minimal variation, suggesting that lighter models possess sufficient capacity to process the provided documentation effectively.

\subsection{Batch Size Analysis}

Our investigation into testing methodology revealed that batch size significantly influences performance consistency. When varying batch sizes across the 130-test suite, results fluctuated erratically.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{traversal_split_boxplot.png}
\caption{Boxplot showing score distribution across different batch sizes.}
\label{fig:batch_boxplot}
\end{figure}

Analysis showed a bell-curve relationship between batch size and performance, with the traversal-split condition acting as a negative predictor. Splitting related test categories (like Walkers) across multiple batches disrupted the LLM's ability to maintain context, leading to degraded scores.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{traversal_split_scatter.png}
\caption{Scatter plot correlating traversal splits with score degradation.}
\label{fig:batch_scatter}
\end{figure}

\subsection{Comparison with Previous Versions}

To assess the effectiveness of the unified v4.9 documentation, we compare it against the previous mini and core variants:

\begin{table}[htbp]
\caption{Documentation Version Comparison}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Mini v3} & \textbf{Core v3} & \textbf{Unified v4.9} \\
\midrule
Claude Sonnet 4.5 & 79.4\% & 79.6\% & 72.9\% \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Note: Direct comparison is limited as v4.9 uses a modified 120-test suite (max 3900 points) while v3 used a 130-test suite (max 4545 points). The unified v4.9 documentation at 2,612 tokens represents a 2.4x increase over mini (1,073 tokens) and a 3.2x reduction from core (8,340 tokens).

\section{Discussion}

\subsection{Compiler-Based Validation Beyond Pattern Matching}

Our evaluation demonstrates that pattern matching alone provides insufficient quality metrics. A model can include all required keywords in syntactically broken code and score high, or write working code using different syntax patterns and get penalized. Pattern matching validates structural intent but cannot guarantee code correctness.

The integration of the Jac compiler (\texttt{jac check}) into the evaluation pipeline significantly improved measurement accuracy. Compiler validation catches errors that pattern matching cannot detect: mismatched brackets, incorrect indentation, malformed expressions, and type errors. This two-stage approach separates structural intent from syntactic correctness, exposing documentation weaknesses that pure pattern matching misses. We conclude that compiler-based validation is essential for meaningful documentation quality assessment.

\subsection{Documentation Size and Comprehension Quality}

The relationship between documentation size and comprehension quality is nonlinear. Our previous mini documentation (1,073 tokens) excelled at basic syntax but lacked sufficient context for complex integrations. Core documentation (8,340 tokens) provided comprehensive explanations but appeared to overwhelm models on simpler tasks, potentially introducing noise that hindered pattern extraction.

The unified v4.9 documentation (2,612 tokens) occupies an intermediate position, providing more context than mini without the verbosity of core. Performance within model families (lite vs full versions) showed minimal variation, suggesting that at this documentation size, model capacity is not the limiting factor. This implies that larger models may benefit from additional documentation content, presenting an opportunity for future investigation into model-size-dependent documentation optimization.

\subsection{Batch Size Effects and Context Leakage}

Analysis of batch size effects revealed that batched evaluation introduces in-context learning artifacts. When semantically related tests share context (e.g., multiple walker tests in the same batch), models infer patterns from preceding examples rather than relying solely on documentation. This manifests as a bell-curve relationship between batch size and performance, with ``traversal-split'' (category splitting across batches) acting as a negative predictor.

To isolate documentation quality from in-context adaptation, we recommend establishing true baselines using batch size 1 (stateless evaluation), then applying measured offsets to batched results for cost-effective approximation of stateless performance.

\subsection{Cross-Model Consistency}

Claude models consistently outperformed other architectures by 5+ percentage points. This performance gap may reflect: (1) superior instruction-following capabilities on technical tasks, (2) better handling of structured documentation formats, or (3) alignment of documentation generation (via Gemini 2.5 Flash) with Claude's processing patterns. Further investigation with documentation generated by diverse models would clarify whether this represents model capability differences or documentation bias.

\subsection{Implications for Documentation Design}

These findings suggest several design principles:

\begin{itemize}
    \item \textbf{Compiler Integration}: Pattern matching provides rapid filtering, but compiler-based validation is essential for accuracy.
    \item \textbf{Intermediate Compression}: Aggressive compression (mini) sacrifices context needed for complex tasks, while excessive comprehensiveness (core) introduces noise. Documentation should target 2-3K tokens for balanced coverage.
    \item \textbf{Multi-Model Validation}: Cross-architecture evaluation exposes model-specific biases and ensures generalizability.
    \item \textbf{Stateless Baselines}: Batch size 1 evaluation provides true documentation quality metrics, free from in-context learning confounds.
\end{itemize}


\section{Reproducibility}

The complete system is available open-source at \url{https://github.com/kevinjin420/jaseci-llmdocs}, comprising:

\begin{itemize}
    \item \textbf{DocGen}: Automated documentation generation pipeline with configurable topics, compression levels, and LLM backends
    \item \textbf{DocBench}: Flask + React evaluation framework with 120-test suite, two-stage validation (pattern matching + compiler checks), OpenRouter integration, and PostgreSQL result storage. Infrastructure for functional testing exists but is not currently utilized.
    \item \textbf{Documentation Artifacts}: All generated documentation versions (v3 mini/core, v4.9 unified)
    \item \textbf{Experimental Data}: Complete benchmark results, category breakdowns, and statistical analyses
\end{itemize}

Detailed usage documentation is available at \url{https://docs.jaseci.org/learn/tools/llmdocs/}.

\section{Conclusion}

We presented a comprehensive framework for generating and evaluating LLM-optimized documentation for emerging programming languages. The automated generation pipeline systematically processes large documentation sets through topic extraction, merging, and hierarchical reduction, producing token-efficient references in 30-45 minutes. The two-stage evaluation framework validates code correctness through pattern matching and compiler-based syntax checking.

Applied to the Jac programming language, our unified v4.9 documentation (2,612 tokens) achieved 68-73\% accuracy across six frontier models. Key findings include: (1) compiler-based validation catches errors that pattern matching alone cannot detect, providing more accurate quality metrics, (2) documentation size exhibits a nonlinear relationship with comprehension quality, with intermediate compression (2-3K tokens) balancing context and clarity, (3) batched evaluation introduces in-context learning artifacts that inflate scores by 5-15\%, requiring stateless baseline correction for true quality measurement, and (4) Claude models consistently outperform other architectures by 5+ percentage points, potentially reflecting superior instruction-following or documentation generation bias.

These findings establish design principles for LLM-optimized documentation: integrate compilers for validation, target intermediate compression levels, validate across multiple architectures, and establish stateless baselines. The open-source tooling enables systematic evaluation and iterative refinement for any emerging language seeking to enable LLM-assisted development.

Future work includes: (1) expanding the test suite to include functional testing with \texttt{jac test} for semantic correctness validation beyond syntax, (2) investigating documentation generation with diverse LLMs to isolate model-specific biases, (3) developing adaptive compression strategies that scale documentation size to model capacity, (4) automating test generation for broader coverage, and (5) extending the framework to additional programming languages to assess generalizability.

\section*{Acknowledgment}

Thanks to llm-docs.com \cite{b1} for the LLMDocs concept that inspired this work, and to the Jaseci Labs team for guidance.

\begin{thebibliography}{00}
\bibitem{b1} LLMDocs, ``LLM-Optimized Documentation,'' \url{https://llm-docs.com/}, 2024.
\bibitem{b2} Jaseci Labs, ``Jac Programming Language,'' \url{https://docs.jaseci.org/}, 2024.
\bibitem{b3} S. Jin, ``jaseci-llmdocs,'' GitHub, \url{https://github.com/kevinjin420/jaseci-llmdocs}, 2024.
\bibitem{b4} Anthropic, ``Claude Models,'' \url{https://www.anthropic.com/claude}, 2024.
\end{thebibliography}

\end{document}
