\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{listings}
\lstset{basicstyle=\ttfamily\small,breaklines=true}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{A Benchmark Framework for Evaluating LLM Comprehension of Emerging Languages}

\author{\IEEEauthorblockN{Shengran Jin\IEEEauthorrefmark{1}\IEEEauthorrefmark{2} and
Ibrahim Musaddequr Rahman\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}University of Michigan}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Email: [Email Placeholder]}
\thanks{This work was conducted at the University of Michigan.}
}

\maketitle

\begin{abstract}
Large Language Models cannot generate accurate code for languages absent from their training data. We present a methodology for designing and evaluating LLM-optimized documentation, supported by a benchmark framework consisting of a 120-test evaluation suite. Applied to the Jac programming language, our evaluation reveals that documentation generated by an LLM exhibits significant bias toward the generating model. Mini documentation (1,390 tokens) created with Claude Sonnet 4.5 achieved a pass rate of 79.4\% on Claude (compared to a 59.7\% baseline) but degraded to 65.5\% on GPT-4o and 64.2\% on Gemini 2.5 Pro. In contrast, comprehensive documentation maintained consistent performance (76-79\%) across all models. These findings demonstrate that cross-model validation is essential for generalizable documentation. We provide open-source tooling for reproducible evaluation and iterative refinement of documentation for emerging languages.
\end{abstract}

\section{Introduction}

LLM-powered coding assistants struggle with languages released after their training cutoff, posing a barrier to emerging language ecosystems. While LLMDocs offers token-efficient documentation, systematic methods for evaluating and improving its effectiveness for LLMs are lacking.

This paper presents a benchmark framework designed to address these challenges. It features a comprehensive test suite and an automated execution system for assessing LLM comprehension of new languages. We introduce a novel baseline correction methodology that isolates true documentation quality from in-context learning artifacts. Additionally, we provide empirical evidence of model-specific bias, underscoring the need for cross-model validation. The framework's utility is demonstrated through its application to the Jac programming language.

\section{Problem Definition}

Given a programming language $L$ for which LLMs have limited pre-training knowledge and documentation $D$ describing $L$, we seek to quantify the LLM's ability to generate correct code in $L$ after reading $D$, identify weaknesses in $D$ to guide improvement, and compare documentation variants to optimize the accuracy/token trade-off.

We define the evaluation objective as a scoring function $S(D, M, T)$ where $D$ is documentation, $M$ is an LLM, and $T$ is a test suite:

\begin{equation}
    S(D, M, T) = \sum_{t \in T} \text{score}(M(D, t), t)
\end{equation}

where $M(D, t)$ is the code generated by model $M$ given documentation $D$ and test prompt $t$.

\section{Benchmark Framework}

We developed a comprehensive benchmark framework consisting of a 120-test evaluation suite designed to measure LLM comprehension of the Jac language. The suite covers 64 distinct categories across 10 difficulty levels, with 42\% of the scoring weight allocated to production-grade scenarios (Level 10) to emphasize real-world mastery. The evaluation mechanism employs a dual-validation strategy: strict pattern matching for required syntactic elements and execution of the official Jac compiler to ensure semantic and syntactic correctness. To isolate true documentation quality from in-context learning artifacts, the system utilizes a ``Baseline Correction'' methodology, comparing single-shot (batch size 1) performance against batched executions to calculate a true documentation effectiveness score.

\section{Iterative Development Methodology}

\subsection{Two-Pass Documentation Generation}

We generate documentation in two passes:

\textbf{Pass 1 (Extraction)}: Feed source documentation to an LLM, generating unrefined docs focusing on specific aspects (syntax, objects, graphs, walkers, AI integration).

\textbf{Pass 2 (Refinement)}: In a fresh context, have an LLM condense the drafts, eliminating redundancy and extracting essential patterns.

\textbf{I AM ALMOST CERTAINLY GOING TO LEARN DISTILLATION AND CHANGE THIS}

\subsection{Benchmark-Driven Refinement}

Our key methodological contribution is using benchmark results to guide documentation improvement:

\begin{enumerate}
    \item Run benchmark on current documentation
    \item Identify lowest-scoring categories
    \item Generate targeted documentation patches for weak areas
    \item Integrate patches into documentation
    \item Re-run benchmark to validate improvement
\end{enumerate}

\subsection{Documentation Variants}

Following iterative refinement, we produced two documentation tiers:

\begin{table}[htbp]
\caption{Documentation Variants}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Variant} & \textbf{Size} & \textbf{Tokens} \\
\midrule
Core & 32.6 KB & 8,700 \\
Mini & 4.2 KB & 1,390 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Core documentation provides comprehensive syntax references with explanations. Mini documentation aggressively compresses to essential patterns only.

\section{Experimental Results}

\subsection{Baseline}

Without documentation, Claude Sonnet 4.5 achieved a baseline score of 59.7\% (2715/4545 points). Categories similar to other languages (Control Flow, Collections) reached 100\%, while Jac-specific features (Cloud, AI Integration) scored 40-50\%.

\subsection{Cross-Model Evaluation}

Our benchmark framework uses stateless queries via OpenRouter, a centralized API and billing service that routes requests to various LLM providers. This ensures reproducible, isolated evaluations without conversational context contamination.

\begin{table}[htbp]
\caption{Cross-Model Comprehension Scores}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Baseline} & \textbf{Mini v3} & \textbf{Core v3} \\
\midrule
Claude Sonnet 4.5 & 59.7\% & 79.4\% & 79.6\% \\
Gemini 2.5 Pro & -- & 64.2\% & 76.1\% \\
GPT-4o & -- & 65.5\% & 79.4\% \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Batch Size Analysis}

Our investigation into testing methodology revealed that batch size significantly influences performance consistency. When varying batch sizes across the 130-test suite, results fluctuated erratically.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{traversal_split_boxplot.png}
\caption{Boxplot showing score distribution across different batch sizes.}
\label{fig:batch_boxplot}
\end{figure}

Analysis showed a bell-curve relationship between batch size and performance, with the traversal-split condition acting as a negative predictor. Splitting related test categories (like Walkers) across multiple batches disrupted the LLM's ability to maintain context, leading to degraded scores.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{traversal_split_scatter.png}
\caption{Scatter plot correlating traversal splits with score degradation.}
\label{fig:batch_scatter}
\end{figure}

\subsection{Model-Specific Bias}

The results reveal a striking pattern. Both documentation variants were generated using Claude Sonnet 4.5. While mini documentation achieves near-parity with core on the generating model, it degrades substantially on others (Table \ref{tab:bias}).

\begin{table}[htbp]
\caption{Impact of Documentation Bias (Mini vs Core)}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Mini v3} & \textbf{Core v3} & \textbf{Degradation} \\
\midrule
Claude Sonnet 4.5 & 79.4\% & 79.6\% & -0.2\% \\
GPT-4o & 65.5\% & 79.4\% & -13.9\% \\
Gemini 2.5 Pro & 64.2\% & 76.1\% & -11.9\% \\
\bottomrule
\end{tabular}
\label{tab:bias}
\end{center}
\end{table}

Core documentation transfers better across architectures, whereas token-efficient mini documentation appears to encode Claude-specific patterns that other models fail to leverage.

\subsection{Consistency}

\textbf{CURRENT VERY POOR, NEED EXTENSIVE TESTING WITH NEW FRAMEWORK TO COMMENT}

\section{Discussion}

\subsection{Methodological Evolution: The Case for True Baselines}

Our initial investigation into evaluation methodology revealed a critical flaw in standard batched benchmarking: in-context learning significantly distorts documentation quality metrics. Analysis of batch sizes ranging from 1 to 65 showed that placing semantically related tests in the same context window (e.g., multiple ``File Operations'' tests) allowed the LLM to infer patterns from sibling examples rather than relying solely on the provided documentation. This resulted in score inflation of 5-15\%, effectively masking deficiencies in the documentation itself.

To address this, we transitioned to a ``True Baseline'' methodology. By establishing a baseline score using a batch size of 1 (isolating each test case), we eliminate context leakage and measure the model's unassisted comprehension. The difference between the batched score and this baseline is defined as the ``Learning Bonus''---a metric representing the model's adaptability rather than the documentation's quality. This distinction is crucial: optimizing for high batched scores often leads to overfitting the model's learning capacity, whereas optimizing for high baseline scores ensures the documentation itself is robust and transferrable.

\subsection{The Model-Specific Bias Problem}

Our central finding is that LLM-generated documentation encodes biases toward the generating model. When Claude Sonnet 4.5 compresses documentation, it retains patterns that Claude itself processes effectively but that other architectures struggle to interpret.

This manifests clearly in our results: mini documentation achieves near-parity with core on Claude (the generating model), but suffers 12-14 percentage point degradation on GPT-4o and Gemini. The compression process, while producing valid and readable documentation, implicitly optimizes for the compressing model's internal representations.

\subsection{Why Core Transfers Better}

Core documentation's superior cross-model performance likely stems from its comprehensiveness. By including more explicit explanations, examples, and context, it provides redundant signals that different model architectures can leverage through their varying attention patterns and tokenization schemes.

Mini documentation's aggressive compression eliminates this redundancy, retaining only patterns the generating model deemed essential. This creates a form of over-optimization to the generator's architecture.

\subsection{Implications for Documentation Design}

These findings suggest that LLM-optimized documentation development should:
\begin{itemize}
    \item Validate across multiple model architectures, not just the generating model
    \item Prefer comprehensive documentation when cross-model compatibility is required
    \item \textbf{MORE STUFF HERE, NEED MORE TESTING TO COMMENT}
\end{itemize}


\section{Reproducibility}

The complete benchmark framework, test suite, and experimental data are available open-source at \url{https://github.com/kevinjin420/jaseci-llmdocs}. Detailed documentation is available at \url{https://docs.jaseci.org/learn/tools/llmdocs/}.

\section{Conclusion}

% We presented a benchmark framework for evaluating LLM comprehension of new programming languages. Applied to Jac, our cross-model evaluation revealed that documentation generated by one LLM exhibits significant bias toward that model. Mini documentation (1,390 tokens) generated with Claude Sonnet 4.5 achieved near-parity with core documentation on Claude itself, but degraded by 12-14 percentage points on GPT-4o and Gemini 2.5 Pro.

% Crucially, our methodological analysis demonstrates that standard batched evaluation conflates documentation quality with in-context learning. By adopting a baseline correction approach, we can isolate true documentation effectiveness, exposing biases that would otherwise go undetected. The open-source tooling provided enables this systematic evaluation for any emerging language seeking to enable LLM-assisted development.

% Future work includes investigating whether documentation generated by different models exhibits similar biases, developing model-agnostic compression techniques, and automated test generation for broader coverage.

\textbf{TO BE WRITTEN}

\section*{Acknowledgment}

Thanks to llm-docs.com \cite{b1} for the LLMDocs concept that inspired this work, and to the Jaseci Labs team for guidance.
Be
\begin{thebibliography}{00}
\bibitem{b1} LLMDocs, ``LLM-Optimized Documentation,'' \url{https://llm-docs.com/}, 2024.
\bibitem{b2} Jaseci Labs, ``Jac Programming Language,'' \url{https://docs.jaseci.org/}, 2024.
\bibitem{b3} S. Jin, ``jaseci-llmdocs,'' GitHub, \url{https://github.com/kevinjin420/jaseci-llmdocs}, 2024.
\bibitem{b4} Anthropic, ``Claude Models,'' \url{https://www.anthropic.com/claude}, 2024.
\end{thebibliography}

\end{document}
